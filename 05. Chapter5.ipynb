{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM39nLRlLY492iXNywxhATf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/05.%20Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 5: Support Vector Machines (SVM)\n",
        "\n",
        "Bab ini memperkenalkan **Support Vector Machines (SVM)**, sebuah model yang sangat kuat dan serbaguna yang dapat melakukan klasifikasi linier maupun non-linier, regresi, dan bahkan deteksi *outlier*.\n",
        "\n",
        "* **Ide Fundamental SVM**: Konsep utama di balik SVM adalah **klasifikasi margin besar** (*large margin classification*). SVM mencoba menemukan \"jalan\" (*street*) terluas yang memisahkan dua kelas. Tujuannya adalah untuk memaksimalkan *margin*, yaitu jarak antara batas keputusan dan instance terdekat dari setiap kelas. Instance yang berada di tepi \"jalan\" ini disebut **support vectors**, karena merekalah yang \"mendukung\" atau menentukan batas keputusan.\n",
        "\n",
        "* **Hard Margin vs. Soft Margin**:\n",
        "    * ***Hard Margin Classification***: Mengharuskan semua instance berada di luar \"jalan\" dan di sisi yang benar. Pendekatan ini hanya berfungsi jika data dapat dipisahkan secara linier dan sangat sensitif terhadap *outlier*.\n",
        "    * ***Soft Margin Classification***: Pendekatan yang lebih fleksibel dengan mencari keseimbangan antara menjaga \"jalan\" selebar mungkin dan membatasi pelanggaran margin (*margin violations*), yaitu instance yang berakhir di tengah jalan atau bahkan di sisi yang salah. Tingkat fleksibilitas ini dikendalikan oleh *hyperparameter* `C`.\n",
        "\n",
        "* **Klasifikasi SVM Non-Linier**:\n",
        "    * Untuk data yang tidak dapat dipisahkan secara linier, SVM dapat menggunakan trik matematika yang disebut **kernel trick**. Trik ini memungkinkan SVM untuk mendapatkan hasil yang sama seolah-olah kita menambahkan banyak fitur non-linier (seperti fitur polinomial tingkat tinggi) tanpa benar-benar harus melakukannya, sehingga menghindari ledakan jumlah fitur secara komputasi.\n",
        "    * **Kernel Populer**:\n",
        "        * ***Polynomial Kernel***: Untuk data dengan pola polinomial.\n",
        "        * ***Gaussian RBF Kernel***: Kernel yang sangat kuat dan berfungsi dengan baik pada berbagai jenis dataset.\n",
        "\n",
        "* **Regresi SVM**:\n",
        "    * SVM juga dapat digunakan untuk tugas regresi. Tujuannya dibalik: alih-alih mencoba membuat \"jalan\" terluas yang memisahkan dua kelas, SVM Regresi mencoba memasukkan sebanyak mungkin instance *ke dalam* \"jalan\" sambil membatasi pelanggaran margin (instance di luar jalan).\n",
        "    * Lebar \"jalan\" ini dikendalikan oleh *hyperparameter* `epsilon` (ε).\n",
        "\n",
        "### 1. Klasifikasi SVM Linier\n",
        "Kita akan menggunakan dataset Iris untuk mendemonstrasikan SVM. Kita akan membuat pengklasifikasi biner untuk mendeteksi apakah bunga adalah *Iris-Virginica* atau bukan.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Memuat dataset Iris\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)  # 1.0 jika Iris-Virginica, else 0.0\n",
        "\n",
        "# Membuat pipeline dengan scaler dan model LinearSVC\n",
        "# Hyperparameter C mengontrol trade-off margin\n",
        "# Nilai C yang lebih kecil menghasilkan margin yang lebih lebar tetapi lebih banyak pelanggaran\n",
        "svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
        "    ])\n",
        "\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "# Membuat prediksi\n",
        "print(\"Prediksi untuk petal length=5.5, petal width=1.7:\", svm_clf.predict([[5.5, 1.7]]))\n",
        "```\n",
        "\n",
        "### 2. Klasifikasi SVM Non-Linier dengan Kernel\n",
        "Untuk data non-linier, kita bisa menggunakan `SVC` dengan kernel. Kita akan menggunakan dataset `moons` sebagai contoh.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat dataset moons\n",
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "def plot_dataset(X, y, axes):\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "\n",
        "# --- Model dengan Kernel Polinomial ---\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "    ])\n",
        "poly_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "\n",
        "# --- Model dengan Kernel Gaussian RBF ---\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "    ])\n",
        "rbf_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "# Catatan: Kode untuk mem-plot decision boundary tidak ditampilkan di sini agar ringkas,\n",
        "# namun tersedia di notebook Jupyter dari buku.\n",
        "print(\"Model dengan kernel polinomial dan RBF telah dilatih.\")\n",
        "```\n",
        "\n",
        "### 3. Regresi SVM\n",
        "SVM juga bisa digunakan untuk regresi. `SVR` adalah kelas yang setara dengan `SVC` untuk regresi.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Membuat data acak non-linier\n",
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1) - 1\n",
        "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1) / 10).ravel()\n",
        "\n",
        "# Melatih model SVR dengan kernel polinomial\n",
        "# epsilon (ε) mengontrol lebar \"jalan\"\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X, y)\n",
        "\n",
        "# Melatih model SVR lain dengan kernel RBF\n",
        "svm_rbf_reg = SVR(kernel=\"rbf\", C=0.1, gamma=0.1)\n",
        "svm_rbf_reg.fit(X, y)\n",
        "\n",
        "\n",
        "def plot_svm_regression(svm_reg, X, y, axes):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
        "    y_pred = svm_reg.predict(x1s)\n",
        "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
        "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
        "    plt.plot(X, y, \"bo\")\n",
        "    plt.axis(axes)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.subplot(121)\n",
        "plot_svm_regression(svm_poly_reg, X, y, [-1, 1, 0, 1])\n",
        "plt.title(\"SVR dengan Kernel Polinomial (degree=2)\")\n",
        "plt.ylabel(r\"$y$\")\n",
        "\n",
        "plt.subplot(122)\n",
        "plot_svm_regression(svm_rbf_reg, X, y, [-1, 1, 0, 1])\n",
        "plt.title(\"SVR dengan Kernel RBF (C=0.1, gamma=0.1)\")\n",
        "plt.show()\n",
        "```\n",
        "Plot di atas menunjukkan bagaimana SVR mencoba memasukkan sebanyak mungkin instance ke dalam \"jalan\" (di antara garis putus-putus) yang lebarnya dikontrol oleh `epsilon`.\n",
        "\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3Yo2oBPqm7dkiScZMOo5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/16.%20Chapter16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 16: Natural Language Processing with RNN and Attention\n",
        "\n",
        "Bab ini melanjutkan pembahasan tentang RNN dan menunjukkan bagaimana mereka dapat diterapkan pada tugas-tugas **Natural Language Processing (NLP)**. Selain itu, bab ini memperkenalkan **attention mechanisms** dan arsitektur **Transformer** yang revolusioner.\n",
        "\n",
        "* **Text Generation dengan Character-RNN**:\n",
        "    * **Konsep**: Melatih RNN untuk memprediksi karakter berikutnya dalam sebuah urutan teks. Setelah dilatih, model ini dapat digunakan untuk menghasilkan teks baru, satu karakter pada satu waktu.\n",
        "    * **Dataset**: Bab ini menunjukkan cara mempersiapkan dataset dari teks mentah (seperti karya Shakespeare) dengan mengubahnya menjadi jendela-jendela teks yang tumpang tindih (*overlapping windows*) menggunakan `tf.data`.\n",
        "    * **Stateful RNN**: Diperkenalkan konsep RNN *stateful*, yang mempertahankan *hidden state*-nya di antara *batch* training. Ini memungkinkannya untuk belajar pola jangka panjang, tetapi memerlukan persiapan data yang lebih hati-hati (urutan data harus berkesinambungan).\n",
        "\n",
        "* **Analisis Sentimen**:\n",
        "    * Beralih dari level karakter ke level kata. Model dilatih untuk mengklasifikasikan teks (misalnya, ulasan film) sebagai positif atau negatif.\n",
        "    * **Tokenisasi**: Proses memecah teks menjadi kata-kata atau *subword*. Bab ini menyinggung tantangan tokenisasi untuk berbagai bahasa dan memperkenalkan teknik *subword* modern.\n",
        "    * **Word Embeddings**: Kata-kata direpresentasikan sebagai vektor padat (*dense vector*) yang dapat dilatih. Lapisan `keras.layers.Embedding` digunakan untuk ini.\n",
        "    * **Masking**: Teknik penting untuk menangani urutan dengan panjang yang bervariasi. Dengan menyetel `mask_zero=True` pada lapisan *embedding*, token *padding* (biasanya dengan ID 0) akan diabaikan oleh lapisan-lapisan berikutnya.\n",
        "    * **Menggunakan Pretrained Embeddings**: Untuk meningkatkan performa, terutama dengan data yang sedikit, kita dapat menggunakan *word embeddings* yang sudah dilatih pada korpus teks yang besar (misalnya, dari TensorFlow Hub).\n",
        "\n",
        "* **Neural Machine Translation (NMT)**:\n",
        "    * **Arsitektur Encoder-Decoder**: Arsitektur standar untuk NMT. *Encoder* (RNN) membaca kalimat sumber dan merangkumnya menjadi sebuah vektor konteks (*hidden state* terakhir). *Decoder* (RNN lain) kemudian menggunakan vektor konteks ini untuk menghasilkan terjemahan, kata demi kata.\n",
        "    * **Attention Mechanisms**: Solusi untuk masalah memori jangka panjang pada arsitektur Encoder-Decoder. *Attention* memungkinkan *decoder* untuk fokus pada bagian-bagian yang relevan dari kalimat sumber saat menghasilkan setiap kata terjemahan. Ini secara drastis meningkatkan kualitas terjemahan, terutama untuk kalimat panjang.\n",
        "\n",
        "* **Arsitektur Transformer**:\n",
        "    * Diperkenalkan dalam paper \"Attention Is All You Need\", arsitektur ini sepenuhnya meninggalkan lapisan rekuren dan konvolusional, dan hanya mengandalkan *attention mechanisms*.\n",
        "    * **Positional Embeddings**: Karena model ini tidak memiliki sifat rekuren, ia tidak memiliki informasi urutan. *Positional embeddings* ditambahkan ke *word embeddings* untuk memberikan informasi posisi absolut dan relatif dari setiap kata.\n",
        "    * **Multi-Head Attention**: Komponen inti dari Transformer. Ia memungkinkan model untuk secara bersamaan memperhatikan informasi dari *subspace* representasi yang berbeda. Ini terdiri dari beberapa *Scaled Dot-Product Attention* yang bekerja secara paralel.\n",
        "\n",
        "### 1. Menghasilkan Teks dengan Character-RNN\n",
        "Contoh ini melatih model untuk menghasilkan teks yang mirip tulisan Shakespeare.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Mengunduh dataset karya Shakespeare\n",
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "# Membuat tokenizer tingkat karakter\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)\n",
        "\n",
        "# Mengubah teks menjadi urutan ID karakter\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "max_id = len(tokenizer.word_index)\n",
        "\n",
        "# Mempersiapkan dataset\n",
        "train_size = int(len(encoded) * 0.9)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.shuffle(10000).batch(32)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "# Membangun model Char-RNN\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "# history = model.fit(dataset, epochs=10) # Training bisa memakan waktu lama\n",
        "\n",
        "# Fungsi untuk menghasilkan teks\n",
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    def preprocess(texts):\n",
        "        X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "        return tf.one_hot(X, max_id)\n",
        "    def next_char(text, temperature=1):\n",
        "        X_new = preprocess([text])\n",
        "        y_proba = model.predict(X_new)[0, -1:, :]\n",
        "        rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "        char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "        return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text\n",
        "\n",
        "# print(complete_text(\"To be or not to b\", temperature=0.5)) # Jalankan setelah melatih model\n",
        "print(\"Model Char-RNN telah dibuat. Hapus komentar pada `model.fit` untuk melatihnya.\")\n",
        "```\n",
        "\n",
        "### 2. Analisis Sentimen dengan Embedding dan Masking\n",
        "Model ini mengklasifikasikan ulasan film dari dataset IMDb.\n",
        "\n",
        "```python\n",
        "# Diperlukan untuk Google Colab\n",
        "!pip install -q -U tensorflow_datasets\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Memuat dataset imdb_reviews\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_set = datasets[\"train\"].batch(32).prefetch(1)\n",
        "valid_set = datasets[\"test\"].take(2500).batch(32).prefetch(1) # Ambil sebagian untuk validasi\n",
        "test_set = datasets[\"test\"].skip(2500).batch(32).prefetch(1)\n",
        "\n",
        "# Membuat dan mengadaptasi lapisan TextVectorization\n",
        "vocab_size = 1000\n",
        "text_vec_layer = keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "text_vec_layer.adapt(train_set.map(lambda review, label: review))\n",
        "\n",
        "# Membangun model dengan Embedding dan Masking\n",
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    text_vec_layer,\n",
        "    # mask_zero=True akan mengabaikan token padding (ID=0)\n",
        "    keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
        "print(\"\\nModel analisis sentimen telah dibuat.\")\n",
        "```\n",
        "\n",
        "### 3. Arsitektur Transformer (Konseptual)\n",
        "Transformer tidak lagi menggunakan RNN, melainkan murni *attention*. Komponen kuncinya adalah **Positional Encoding** dan **Multi-Head Attention**.\n",
        "\n",
        "```python\n",
        "# --- Lapisan Positional Encoding Kustom ---\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
        "\n",
        "# --- Lapisan Multi-Head Attention Sederhana ---\n",
        "# Ini adalah versi konseptual, implementasi sebenarnya lebih kompleks\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, causal=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.causal = causal\n",
        "        # Lapisan-lapisan ini akan dibuat di dalam metode build()\n",
        "        # self.q_dense, self.k_dense, self.v_dense, self.attention, self.out_dense\n",
        "    def build(self, batch_input_shape):\n",
        "      # Implementasi build\n",
        "      pass\n",
        "    def call(self, inputs, mask=None):\n",
        "      # Logika call\n",
        "      pass\n",
        "\n",
        "# Contoh kerangka model Transformer Encoder\n",
        "embed_size = 128\n",
        "max_steps = 500\n",
        "vocab_size = 10000\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)(encoder_inputs)\n",
        "encoder_in = PositionalEncoding(max_steps, max_dims=embed_size)(embeddings)\n",
        "\n",
        "# Z = encoder_in\n",
        "# for N in range(6): # Tumpukan 6 blok encoder\n",
        "#    Z = MultiHeadAttention(...)(Z) # Blok attention\n",
        "#    Z = keras.layers.LayerNormalization()(Z)\n",
        "#    ... # Feed Forward Network\n",
        "# encoder_outputs = Z\n",
        "print(\"\\nKerangka konseptual untuk Transformer telah disiapkan.\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
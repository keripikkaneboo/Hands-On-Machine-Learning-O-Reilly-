{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxrS7/c+UsSYQFZnsoXdYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/12.%20Chapter12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 12: Costum Models and Training with TensorFlow\n",
        "\n",
        "Meskipun API tingkat tinggi seperti `tf.keras` sudah sangat kuat, terkadang kita memerlukan kontrol lebih besar untuk mengimplementasikan arsitektur yang tidak biasa, *loss function* kustom, atau *training loop* yang spesifik. Bab ini membahas cara menggunakan API tingkat rendah TensorFlow untuk mendapatkan fleksibilitas tersebut.\n",
        "\n",
        "* **TensorFlow Seperti NumPy**: Bab ini dimulai dengan menunjukkan bahwa TensorFlow pada dasarnya adalah library komputasi numerik yang sangat kuat, mirip dengan NumPy tetapi dengan dukungan GPU dan kemampuan untuk menghasilkan grafik komputasi.\n",
        "    * ***Tensor***: Struktur data fundamental di TensorFlow, mirip dengan `ndarray` NumPy tetapi bersifat *immutable* (tidak dapat diubah).\n",
        "    * ***Variable***: Digunakan untuk menyimpan parameter model yang dapat diubah (misalnya, bobot dan bias) selama training.\n",
        "    * **Operasi**: TensorFlow menyediakan banyak sekali operasi matematika yang dapat dijalankan pada tensor.\n",
        "\n",
        "* **Komponen Kustom di Keras**: Anda dapat mengkustomisasi hampir semua bagian dari Keras:\n",
        "    * ***Loss Functions***: Anda bisa membuat fungsi Python sederhana atau membuat *class* yang mewarisi dari `keras.losses.Loss` jika Anda memerlukan *hyperparameter* atau *state*.\n",
        "    * ***Metrics***: Serupa dengan *loss*, tetapi metrik digunakan untuk evaluasi. Untuk metrik yang nilainya perlu diakumulasi sepanjang *epoch* (seperti *precision*), Anda perlu membuat *class* yang mewarisi dari `keras.metrics.Metric` dan mengimplementasikan metode `update_state()` dan `result()`. Ini disebut **streaming metric**.\n",
        "    * ***Layers***: Anda dapat membuat *layer* kustom dengan mewarisi dari `keras.layers.Layer` dan mengimplementasikan metode `build()` (untuk membuat bobot) dan `call()` (untuk logika *forward pass*).\n",
        "    * ***Models***: Untuk arsitektur yang sangat kompleks dan dinamis (misalnya, yang berisi perulangan atau logika kondisional), Anda bisa membuat model kustom dengan mewarisi dari `keras.Model`.\n",
        "\n",
        "* **Menghitung Gradien dengan Autodiff**:\n",
        "    * **`tf.GradientTape`**: Alat utama TensorFlow untuk diferensiasi otomatis (*autodiff*). Ia merekam semua operasi yang melibatkan *variable* di dalam blok `with`, lalu dapat menghitung gradien dari sebuah *output* terhadap *variable-variable* tersebut.\n",
        "\n",
        "* ***Custom Training Loop***:\n",
        "    * Meskipun metode `fit()` sangat praktis, terkadang Anda perlu menulis *training loop* sendiri. Ini memberi Anda kontrol penuh atas proses training.\n",
        "    * Langkah-langkahnya meliputi: iterasi manual melalui *epoch* dan *batch*, melakukan *forward pass* di dalam `tf.GradientTape`, menghitung *loss*, menghitung gradien, dan terakhir menerapkan gradien tersebut menggunakan optimizer.\n",
        "\n",
        "* **TF Functions dan Graphs**:\n",
        "    * ***`@tf.function`***: Sebuah *decorator* yang mengubah fungsi Python menjadi **TensorFlow Function (TF Function)** yang dapat dieksekusi dengan performa tinggi.\n",
        "    * **AutoGraph dan Tracing**: Di balik layar, `@tf.function` menganalisis kode Python (*AutoGraph*) dan membuat grafik komputasi yang dioptimalkan untuk *input signature* tertentu (*tracing*). Hal ini membuat kode berjalan jauh lebih cepat dan portabel.\n",
        "\n",
        "### 1. Custom Loss Function\n",
        "Contoh membuat Huber loss sebagai sebuah *class* agar *hyperparameter* `threshold` dapat disimpan bersama model.\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "# Contoh penggunaan:\n",
        "# model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\")\n",
        "```\n",
        "\n",
        "### 2. Custom Layer\n",
        "Contoh membuat lapisan `Dense` versi sederhana.\n",
        "\n",
        "```python\n",
        "class MyDense(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        # Metode build() dipanggil saat pertama kali layer digunakan untuk membuat bobot\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
        "            initializer=\"glorot_normal\")\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
        "        super().build(batch_input_shape) # Wajib di akhir\n",
        "\n",
        "    def call(self, X):\n",
        "        # Logika forward pass\n",
        "        return self.activation(X @ self.kernel + self.bias)\n",
        "```\n",
        "\n",
        "### 3. Menghitung Gradien dengan `tf.GradientTape`\n",
        "Contoh sederhana untuk menghitung gradien dari sebuah fungsi.\n",
        "\n",
        "```python\n",
        "# Fungsi sederhana: f(w1, w2) = 3 * w1**2 + 2 * w1 * w2\n",
        "def f(w1, w2):\n",
        "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
        "\n",
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = f(w1, w2)\n",
        "\n",
        "# Menghitung gradien z terhadap w1 dan w2\n",
        "gradients = tape.gradient(z, [w1, w2])\n",
        "\n",
        "print(\"Gradien:\", gradients)\n",
        "```\n",
        "\n",
        "### 4. Custom Training Loop\n",
        "Kerangka dasar untuk sebuah *custom training loop*.\n",
        "\n",
        "```python\n",
        "# 1. Membuat model sederhana (tanpa compile)\n",
        "l2_reg = keras.regularizers.l2(0.05)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                       kernel_regularizer=l2_reg),\n",
        "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
        "])\n",
        "\n",
        "# 2. Menyiapkan data & hyperparameter (contoh dummy)\n",
        "# Misalkan X_train_scaled dan y_train sudah disiapkan\n",
        "# X_train_scaled, y_train = ...\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps = len(X_train) // batch_size\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "# 3. Training Loop\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "    for step in range(1, n_steps + 1):\n",
        "        # Ambil satu batch data\n",
        "        # X_batch, y_batch = ...\n",
        "\n",
        "        # Gunakan GradientTape untuk menghitung gradien\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     y_pred = model(X_batch, training=True)\n",
        "        #     main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        #     # Menambahkan regularization loss dari model\n",
        "        #     loss = tf.add_n([main_loss] + model.losses)\n",
        "\n",
        "        # gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        \n",
        "        # # Menerapkan gradien ke optimizer\n",
        "        # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        pass # Komentari pass dan un-comment kode di atas saat dijalankan\n",
        "\n",
        "print(\"\\nTraining selesai (kode training loop di-comment agar tidak error saat dijalankan tanpa data).\")\n",
        "```\n",
        "**Catatan**: Kode *training loop* di atas hanya kerangka. Anda perlu menyediakan data (`X_train`, `y_train`, `X_batch`, `y_batch`) agar bisa berjalan penuh.\n",
        "\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
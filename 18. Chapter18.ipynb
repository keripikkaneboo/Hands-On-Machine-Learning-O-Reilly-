{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI+kWMieiWlSvdYPzEEU4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/18.%20Chapter18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 18: Reinforcement Learning\n",
        "\n",
        "Bab ini membahas **Reinforcement Learning (RL)**, sebuah cabang Machine Learning di mana sebuah *software agent* belajar untuk mengambil tindakan dalam sebuah *environment* untuk memaksimalkan *reward* kumulatif dari waktu ke waktu.\n",
        "\n",
        "* **Konsep Inti RL**:\n",
        "    * ***Agent***: Entitas yang belajar dan membuat keputusan (misalnya, program yang mengontrol karakter game).\n",
        "    * ***Environment***: Dunia di mana agent beroperasi (misalnya, game Atari).\n",
        "    * ***Action***: Keputusan yang diambil oleh agent.\n",
        "    * ***Observation***: Informasi yang diterima agent dari environment.\n",
        "    * ***Reward***: Umpan balik (positif atau negatif) yang diterima agent setelah mengambil tindakan.\n",
        "    * ***Policy***: Strategi yang digunakan agent untuk memilih tindakan berdasarkan observasi. Tujuannya adalah untuk menemukan *policy* yang memaksimalkan total *reward*.\n",
        "\n",
        "* **OpenAI Gym**: Sebuah *toolkit* yang menyediakan berbagai lingkungan simulasi (game, robotika, dll.) untuk mengembangkan dan membandingkan algoritma RL.\n",
        "\n",
        "* **Tantangan Credit Assignment**: Kesulitan utama dalam RL adalah mengetahui tindakan mana yang menyebabkan sebuah *reward*. Seringkali, *reward* datang dengan penundaan. Solusinya adalah dengan mengevaluasi setiap tindakan berdasarkan jumlah *reward* masa depan yang didiskontokan (*discounted future rewards*).\n",
        "\n",
        "* **Dua Pendekatan Utama**:\n",
        "    1.  **Policy Gradients (PG)**:\n",
        "        * Algoritma yang secara langsung mencoba mengoptimalkan parameter dari sebuah *policy*.\n",
        "        * Jaringan saraf digunakan sebagai *policy network* yang memetakan observasi ke probabilitas tindakan.\n",
        "        * Training dilakukan dengan cara: menjalankan beberapa episode, menghitung *advantage* (keuntungan) dari setiap tindakan, lalu memperbarui bobot jaringan untuk membuat tindakan yang \"baik\" (dengan *advantage* positif) lebih mungkin terjadi di masa depan.\n",
        "    2.  **Q-Learning**:\n",
        "        * Alih-alih belajar *policy* secara langsung, algoritma ini belajar untuk mengestimasi nilai optimal dari setiap pasangan *state-action*, yang disebut **Q-Value**.\n",
        "        * Q-Value `Q(s, a)` merepresentasikan total *reward* masa depan yang diharapkan jika agent berada di *state* `s`, mengambil *action* `a`, dan kemudian bertindak secara optimal.\n",
        "        * ***Deep Q-Network (DQN)***: Menggunakan jaringan saraf dalam untuk mengaproksimasi Q-Values, yang memungkinkan penerapan pada lingkungan dengan *state space* yang sangat besar (seperti game dari layar piksel mentah).\n",
        "        * ***Replay Buffer (Experience Replay)***: Teknik krusial di mana pengalaman (transisi *state*, *action*, *reward*) disimpan dalam sebuah buffer. Model dilatih pada sampel acak dari buffer ini untuk mengurangi korelasi antar pengalaman dan menstabilkan training.\n",
        "\n",
        "* **TF-Agents**: Library tingkat tinggi dari Google untuk RL yang menyederhanakan implementasi agen kompleks, lingkungan, dan *training loop*.\n",
        "\n",
        "### 1. Pengenalan OpenAI Gym\n",
        "Contoh dasar penggunaan lingkungan `CartPole`, di mana tujuannya adalah menyeimbangkan tiang di atas kereta.\n",
        "\n",
        "```python\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Membuat environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "\n",
        "print(\"Initial observation:\", obs)\n",
        "\n",
        "# Menjalankan satu langkah dengan aksi acak (0=kiri, 1=kanan)\n",
        "action = env.action_space.sample()\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(\"New observation:\", obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Done:\", done)\n",
        "\n",
        "env.close()\n",
        "```\n",
        "\n",
        "### 2. Policy Gradients (PG)\n",
        "Kita akan membangun *policy network* sederhana dan melatihnya dengan algoritma REINFORCE.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Membuat Policy Network\n",
        "n_inputs = env.observation_space.shape[0] # 4 untuk CartPole\n",
        "n_outputs = env.action_space.n # 2 untuk CartPole\n",
        "\n",
        "model_pg = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"), # Probabilitas aksi \"kiri\"\n",
        "])\n",
        "\n",
        "# 2. Fungsi untuk menjalankan satu episode dan mengumpulkan reward & gradien\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "    return obs, reward, done, grads\n",
        "\n",
        "# 3. Fungsi untuk menghitung discounted rewards\n",
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "# 4. Fungsi untuk menormalisasi rewards (menjadi advantage)\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted - reward_mean) / reward_std\n",
        "            for discounted in all_discounted_rewards]\n",
        "\n",
        "# Kerangka Training Loop (kode penuh ada di notebook)\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "# loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "# for iteration in range(n_iterations):\n",
        "#     all_rewards, all_grads = play_multiple_episodes(...)\n",
        "#     all_final_rewards = discount_and_normalize_rewards(...)\n",
        "#     # Menghitung gradien rata-rata yang sudah dibobot oleh advantage\n",
        "#     # Menerapkan gradien ke model\n",
        "print(\"Kerangka untuk Policy Gradients telah disiapkan.\")\n",
        "```\n",
        "\n",
        "### 3. Deep Q-Network (DQN)\n",
        "Kita akan membuat DQN untuk masalah yang sama. Kuncinya adalah *replay buffer* dan pembaruan target berdasarkan *Bellman equation*.\n",
        "\n",
        "```python\n",
        "from collections import deque\n",
        "\n",
        "# 1. Membuat Replay Buffer\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "# 2. Membuat Deep Q-Network (DQN)\n",
        "# Model ini akan memprediksi Q-Value untuk setiap aksi\n",
        "model_dqn = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs)\n",
        "])\n",
        "\n",
        "# 3. Fungsi untuk memilih aksi (Epsilon-Greedy Policy)\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model_dqn.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "# 4. Kerangka Training Step\n",
        "# def training_step(batch_size):\n",
        "#     # 1. Sampel pengalaman dari replay buffer\n",
        "#     experiences = sample_experiences(batch_size)\n",
        "#     # 2. Hitung target Q-Values menggunakan Bellman equation\n",
        "#     #    target_Q_values = rewards + discount_factor * np.max(next_Q_values, axis=1)\n",
        "#     # 3. Latih model DQN dengan Gradient Descent pada (target_Q_values - predicted_Q_values)\n",
        "print(\"\\nKerangka untuk Deep Q-Network (DQN) telah disiapkan.\")\n",
        "```\n",
        "Training RL seringkali tidak stabil dan sangat sensitif terhadap *hyperparameter*. Menggunakan library yang sudah matang seperti **TF-Agents** sangat direkomendasikan untuk proyek yang lebih serius karena ia mengimplementasikan banyak perbaikan dan stabilisasi (seperti *Double DQN*, *Dueling DQN*, dll).\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
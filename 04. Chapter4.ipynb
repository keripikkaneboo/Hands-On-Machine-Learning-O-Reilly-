{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNTPWoTtXXRfcHClHZOBqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/04.%20Chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 4: Training Models\n",
        "\n",
        "Bab ini menyelam lebih dalam ke algoritma-algoritma Machine Learning, dengan fokus pada **Model Linier**. Bab ini menjelaskan dua cara yang sangat berbeda untuk melatihnya:\n",
        "1.  Menggunakan solusi matematis langsung (*closed-form equation*).\n",
        "2.  Menggunakan pendekatan optimisasi iteratif (*Gradient Descent*).\n",
        "\n",
        "Selain itu, bab ini juga mencakup regresi polinomial, cara mendiagnosis *overfitting* dan *underfitting* dengan *learning curves*, dan model yang umum digunakan untuk klasifikasi seperti *Logistic Regression* dan *Softmax Regression*.\n",
        "\n",
        "* **Regresi Linier**: Model paling sederhana yang membuat prediksi dengan menghitung jumlah terbobot dari fitur-fitur input, ditambah *bias term*.\n",
        "    * ***Normal Equation***: Sebuah persamaan matematis yang memberikan hasil parameter optimal secara langsung. Sangat efisien untuk dataset yang tidak terlalu besar dan jumlah fitur yang tidak terlalu banyak.\n",
        "    * ***Gradient Descent (GD)***: Algoritma optimisasi generik yang secara bertahap menyesuaikan parameter untuk meminimalkan *cost function*.\n",
        "        * ***Batch GD***: Menggunakan seluruh set data training di setiap langkahnya.\n",
        "        * ***Stochastic GD (SGD)***: Menggunakan hanya satu instance acak di setiap langkah, membuatnya jauh lebih cepat tetapi kurang stabil.\n",
        "        * ***Mini-batch GD***: Kompromi antara keduanya, menggunakan sekelompok kecil instance acak (*mini-batch*) di setiap langkah.\n",
        "\n",
        "* **Regresi Polinomial**: Teknik untuk menggunakan model linier pada data non-linier dengan cara menambahkan pangkat dari fitur sebagai fitur baru.\n",
        "\n",
        "* ***Learning Curves***: Alat untuk mendiagnosis apakah sebuah model mengalami *overfitting* atau *underfitting*. Kurva ini memplot performa model pada *training set* dan *validation set* sebagai fungsi dari ukuran *training set*.\n",
        "\n",
        "* **Model Linier yang Diregularisasi**: Teknik untuk mengurangi *overfitting* dengan membatasi bobot (*weights*) model.\n",
        "    * ***Ridge Regression***: Menambahkan penalti ℓ₂ pada *cost function*.\n",
        "    * ***Lasso Regression***: Menambahkan penalti ℓ₁ yang cenderung membuat bobot fitur yang tidak penting menjadi nol (melakukan seleksi fitur otomatis).\n",
        "    * ***Elastic Net***: Kombinasi dari Ridge dan Lasso.\n",
        "    * ***Early Stopping***: Bentuk regularisasi dengan menghentikan training ketika error pada *validation set* berhenti menurun.\n",
        "\n",
        "* **Regresi Logistik dan Softmax**:\n",
        "    * ***Logistic Regression***: Digunakan untuk tugas klasifikasi biner dengan mengestimasi probabilitas.\n",
        "    * ***Softmax Regression***: Generalisasi dari Regresi Logistik untuk mendukung klasifikasi multikelas.\n",
        "\n",
        "### 1. Regresi Linier dengan Normal Equation\n",
        "Kita bisa menghitung parameter ($\\theta$) optimal secara langsung.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat data linier acak\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Menambahkan x0 = 1 ke setiap instance (untuk bias term)\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "# Menghitung theta terbaik menggunakan Normal Equation\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        "print(\"Theta terbaik yang dihitung dengan Normal Equation:\")\n",
        "print(theta_best)\n",
        "\n",
        "# Membuat prediksi menggunakan theta yang ditemukan\n",
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        "y_predict = X_new_b.dot(theta_best)\n",
        "\n",
        "print(\"\\nPrediksi untuk X_new:\")\n",
        "print(y_predict)\n",
        "\n",
        "# Plot data dan garis regresi\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_predict, \"r-\", label=\"Prediksi\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Melakukan hal yang sama dengan Scikit-Learn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print(\"\\nScikit-Learn intercept dan coefficient:\")\n",
        "print(lin_reg.intercept_, lin_reg.coef_)\n",
        "```\n",
        "\n",
        "### 2. Regresi Linier dengan Batch Gradient Descent\n",
        "Implementasi manual untuk memahami cara kerja algoritma optimisasi iteratif.\n",
        "\n",
        "```python\n",
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100 # jumlah instance\n",
        "\n",
        "theta = np.random.randn(2, 1)  # inisialisasi acak\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "print(\"Theta terbaik yang dihitung dengan Batch Gradient Descent:\")\n",
        "print(theta)\n",
        "```\n",
        "\n",
        "### 3. Regresi Polinomial\n",
        "Menggunakan model linier untuk data non-linier.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Membuat data non-linier\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# Menambahkan fitur polinomial (derajat 2)\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Melatih model Linear Regression pada data yang sudah ditransformasi\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "\n",
        "print(\"Intercept dan Coefficient dari Regresi Polinomial:\")\n",
        "print(lin_reg.intercept_, lin_reg.coef_)\n",
        "\n",
        "# Plot hasil\n",
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Prediksi\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 4. Model yang Diregularisasi\n",
        "Contoh penggunaan Ridge, Lasso, dan Elastic Net untuk mencegah *overfitting*.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "# Menggunakan data yang sama dengan Regresi Polinomial\n",
        "# Misalkan X_poly dan y sudah ada\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
        "ridge_reg.fit(X_poly, y)\n",
        "print(\"Ridge Intercept & Coef:\", ridge_reg.intercept_, ridge_reg.coef_)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X_poly, y)\n",
        "print(\"Lasso Intercept & Coef:\", lasso_reg.intercept_, lasso_reg.coef_)\n",
        "\n",
        "# Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "elastic_net.fit(X_poly, y)\n",
        "print(\"ElasticNet Intercept & Coef:\", elastic_net.intercept_, elastic_net.coef_)\n",
        "```\n",
        "\n",
        "### 5. Regresi Logistik\n",
        "Contoh klasifikasi biner menggunakan dataset Iris.\n",
        "\n",
        "```python\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Memuat dataset Iris\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, 3:]  # fitur petal width\n",
        "y = (iris[\"target\"] == 2).astype(int)  # 1 jika Iris-Virginica, else 0\n",
        "\n",
        "# Melatih model Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "# Membuat prediksi probabilitas\n",
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "# Plot hasil\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Bukan Iris-Virginica\")\n",
        "plt.xlabel(\"Petal width (cm)\")\n",
        "plt.ylabel(\"Probabilitas\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Prediksi kelas\n",
        "print(\"\\nPrediksi untuk petal width 1.7cm:\", log_reg.predict([[1.7]]))\n",
        "print(\"Prediksi untuk petal width 1.5cm:\", log_reg.predict([[1.5]]))\n",
        "```\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
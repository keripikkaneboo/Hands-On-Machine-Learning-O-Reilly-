{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0If3AvEAPYPhugQlq3lp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/08.%20Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 8: Dimensionality Reduction\n",
        "\n",
        "Bab ini membahas masalah yang dikenal sebagai **kutukan dimensi** (*curse of dimensionality*), di mana banyak hal menjadi aneh dan kontra-intuitif di ruang berdimensi tinggi. Data menjadi sangat tersebar, yang membuat pencarian pola menjadi sulit dan meningkatkan risiko *overfitting*. **Reduksi dimensi** adalah serangkaian teknik untuk mengurangi jumlah fitur dalam dataset, dengan tujuan:\n",
        "* Mempercepat algoritma training.\n",
        "* Menghemat ruang penyimpanan.\n",
        "* Memvisualisasikan data berdimensi tinggi dalam 2D atau 3D.\n",
        "\n",
        "* **Pendekatan Utama Reduksi Dimensi**:\n",
        "    * **Proyeksi (*Projection*)**: Memproyeksikan setiap titik data ke *subspace* berdimensi lebih rendah. Bekerja dengan baik jika data terletak di dekat *subspace* yang \"datar\". **Principal Component Analysis (PCA)** adalah contoh paling populer.\n",
        "    * **Pembelajaran Manifold (*Manifold Learning*)**: Berasumsi bahwa sebagian besar dataset dunia nyata terletak di dekat *manifold* berdimensi jauh lebih rendah (bentuk yang bisa ditekuk atau dipelintir di ruang dimensi yang lebih tinggi). Tujuannya adalah untuk \"membuka\" lipatan manifold tersebut. Contohnya adalah **Kernel PCA** dan **Locally Linear Embedding (LLE)**.\n",
        "\n",
        "* **Principal Component Analysis (PCA)**:\n",
        "    * PCA mengidentifikasi sumbu-sumbu (*principal components*) yang mempertahankan varians paling besar dalam data.\n",
        "    * Sumbu pertama adalah arah di mana data paling tersebar, sumbu kedua ortogonal terhadap yang pertama dan mempertahankan sisa varians terbesar, dan seterusnya.\n",
        "    * **Memilih Jumlah Dimensi**: Daripada memilih jumlah dimensi secara sembarangan, kita bisa memilih jumlah dimensi yang mempertahankan sebagian besar varians (misalnya, 95%).\n",
        "    * **Varian PCA**:\n",
        "        * ***Incremental PCA (IPCA)***: Berguna untuk dataset besar yang tidak muat dalam memori (*out-of-core*).\n",
        "        * ***Randomized PCA***: Algoritma stokastik yang menemukan perkiraan PC dengan cepat.\n",
        "\n",
        "* **Kernel PCA (kPCA)**: Menerapkan *kernel trick* (yang kita lihat di Bab 5 tentang SVM) ke PCA. Hal ini memungkinkan PCA untuk melakukan proyeksi non-linier, yang sangat baik untuk data yang terletak pada manifold yang kompleks (seperti \"Swiss roll\").\n",
        "\n",
        "* **Locally Linear Embedding (LLE)**: Teknik *manifold learning* non-linier lain yang tidak mengandalkan proyeksi. LLE bekerja dengan mengidentifikasi hubungan linier lokal setiap instance dengan tetangga terdekatnya, lalu mencoba mereproduksi hubungan tersebut di ruang berdimensi lebih rendah.\n",
        "\n",
        "### 1. Principal Component Analysis (PCA)\n",
        "PCA adalah teknik reduksi dimensi yang paling banyak digunakan. Ia memproyeksikan data ke *hyperplane* yang paling dekat dengan data, sambil memaksimalkan varians.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Memuat dataset MNIST (mengambil subset kecil agar cepat)\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "# Membagi data untuk demonstrasi\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
        "\n",
        "# Inisialisasi PCA untuk mempertahankan 95% dari varians\n",
        "# Scikit-Learn secara otomatis memilih jumlah komponen yang tepat\n",
        "pca = PCA(n_components=0.95)\n",
        "\n",
        "# Melakukan reduksi dimensi pada training set\n",
        "X_train_reduced = pca.fit_transform(X_train)\n",
        "\n",
        "print(\"Jumlah komponen setelah PCA (95% varians):\", pca.n_components_)\n",
        "print(\"Dimensi data asli:\", X_train.shape)\n",
        "print(\"Dimensi data setelah reduksi:\", X_train_reduced.shape)\n",
        "\n",
        "# Kita bisa merekonstruksi data kembali (dengan sedikit kehilangan informasi)\n",
        "X_train_recovered = pca.inverse_transform(X_train_reduced)\n",
        "```\n",
        "Dengan PCA, kita bisa mengurangi jumlah fitur dari 784 menjadi sekitar 154 (bervariasi) sambil tetap mempertahankan 95% informasi (varians) dari dataset.\n",
        "\n",
        "### 2. Kernel PCA (kPCA)\n",
        "kPCA sangat baik untuk dataset non-linier, seperti dataset \"Swiss roll\".\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat dataset Swiss roll\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "\n",
        "# Menggunakan KernelPCA dengan kernel RBF untuk membuka lipatan manifold\n",
        "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "\n",
        "# Plot hasil\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
        "plt.title(\"Hasil kPCA dengan Kernel RBF\")\n",
        "plt.xlabel(\"$z_1$\")\n",
        "plt.ylabel(\"$z_2$\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "Plot di atas menunjukkan bagaimana kPCA berhasil \"membuka\" gulungan Swiss roll menjadi representasi 2D yang jauh lebih berguna daripada proyeksi linier biasa.\n",
        "\n",
        "### 3. Locally Linear Embedding (LLE)\n",
        "LLE adalah pendekatan lain untuk *manifold learning* yang fokus pada hubungan lokal antar instance.\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "# Menggunakan dataset Swiss roll yang sama\n",
        "# n_neighbors adalah hyperparameter penting yang perlu di-tune\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
        "X_reduced_lle = lle.fit_transform(X)\n",
        "\n",
        "# Plot hasil\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_reduced_lle[:, 0], X_reduced_lle[:, 1], c=t, cmap=plt.cm.hot)\n",
        "plt.title(\"Hasil LLE\")\n",
        "plt.xlabel(\"$z_1$\")\n",
        "plt.ylabel(\"$z_2$\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "Sama seperti kPCA, LLE berhasil membuka lipatan Swiss roll. Teknik yang berbeda mungkin bekerja lebih baik pada dataset yang berbeda, jadi seringkali ada baiknya mencoba beberapa pendekatan.\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
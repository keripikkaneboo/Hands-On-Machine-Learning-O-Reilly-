{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtnjz2sNaIckjvqamVDtDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/19.%20Chapter19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Bab 19: Training and Deploying TensorFlow Models at Scale\n",
        "\n",
        "Setelah Anda memiliki model yang bagus, langkah selanjutnya adalah membawanya ke lingkungan produksi. Bab ini membahas cara men-deploy model TensorFlow agar dapat digunakan oleh aplikasi lain dan cara melatih model secara efisien pada dataset besar menggunakan beberapa perangkat.\n",
        "\n",
        "* **Men-deploy Model dengan TensorFlow Serving**:\n",
        "    * **SavedModel Format**: Langkah pertama adalah mengekspor model Anda ke format **SavedModel**. Ini adalah format universal TensorFlow yang menyimpan arsitektur model (grafik komputasi), bobot, dan aset lainnya dalam sebuah direktori.\n",
        "    * **TensorFlow Serving (TF Serving)**: Server produksi berperforma tinggi yang ditulis dalam C++. Fitur utamanya adalah:\n",
        "        * Dapat melayani beberapa model atau versi model secara bersamaan.\n",
        "        * Secara otomatis memuat versi model terbaru dari sebuah direktori.\n",
        "        * Mendukung *batching* permintaan untuk meningkatkan *throughput* pada GPU.\n",
        "        * Dapat di-deploy dengan mudah menggunakan **Docker**.\n",
        "    * **Querying the Service**: Anda dapat berinteraksi dengan TF Serving melalui:\n",
        "        * **REST API**: Mudah digunakan, berbasis JSON, dan kompatibel dengan hampir semua klien.\n",
        "        * **gRPC API**: Lebih efisien karena menggunakan format biner Protobuf, cocok untuk data besar dan latensi rendah.\n",
        "\n",
        "* **Deploy ke Platform Cloud**: Menggunakan layanan cloud seperti **Google Cloud AI Platform** menyederhanakan proses deployment, penskalaan, dan pemantauan. Anda cukup mengunggah SavedModel Anda ke Google Cloud Storage (GCS), lalu membuat model dan versi di AI Platform.\n",
        "\n",
        "* **Deploy ke Perangkat Seluler/Embedded (TFLite)**:\n",
        "    * **TensorFlow Lite (TFLite)** adalah kerangka kerja untuk menjalankan model pada perangkat dengan sumber daya terbatas.\n",
        "    * Tujuannya adalah untuk mengurangi ukuran model dan latensi.\n",
        "    * Teknik yang digunakan termasuk **kuantisasi** (*quantization*), di mana bobot dan aktivasi dikonversi dari float 32-bit menjadi integer 8-bit, yang secara dramatis mengurangi ukuran dan mempercepat inferensi.\n",
        "\n",
        "* **Melatih Model dalam Skala Besar**:\n",
        "    * **Menggunakan GPU**: Merupakan cara paling umum untuk mempercepat training. TensorFlow secara otomatis akan menempatkan komputasi pada GPU jika tersedia.\n",
        "    * **Distribution Strategies API (`tf.distribute`)**: API sederhana dari TensorFlow untuk mendistribusikan training di beberapa perangkat atau server.\n",
        "        * ***Data Parallelism***: Strategi yang paling umum, di mana model direplikasi di setiap perangkat, dan setiap replika dilatih pada *mini-batch* data yang berbeda.\n",
        "        * ***MirroredStrategy***: Untuk training sinkron di beberapa GPU pada satu mesin.\n",
        "        * ***MultiWorkerMirroredStrategy***: Untuk training sinkron di beberapa mesin (*multi-worker*).\n",
        "        * ***ParameterServerStrategy***: Untuk training asinkron di mana parameter disimpan di server terpusat (*parameter servers*).\n",
        "\n",
        "### 1. Mengekspor Model ke Format SavedModel\n",
        "Setiap model Keras dapat dengan mudah disimpan dalam format ini.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "# Misalkan kita punya model yang sudah dilatih\n",
        "# (Gunakan model dari bab sebelumnya atau buat model dummy)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
        "# model.fit(...)\n",
        "\n",
        "# Tentukan path untuk menyimpan model\n",
        "model_name = \"my_keras_model\"\n",
        "model_version = \"0001\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "\n",
        "# Simpan model\n",
        "model.save(model_path)\n",
        "```\n",
        "Setelah menjalankan kode ini, direktori `my_keras_model/0001/` akan dibuat dengan semua file yang diperlukan.\n",
        "\n",
        "### 2. Menjalankan TensorFlow Serving dengan Docker\n",
        "Perintah ini harus dijalankan di terminal lokal Anda (bukan di Colab) setelah menginstal Docker.\n",
        "\n",
        "```bash\n",
        "# Ganti $ML_PATH dengan path ke direktori proyek Anda\n",
        "# Perintah ini akan menjalankan TF Serving di Docker dan memuat model Anda\n",
        "# docker run -it --rm -p 8501:8501 \\\n",
        "#    -v \"$ML_PATH/my_keras_model:/models/my_keras_model\" \\\n",
        "#    -e MODEL_NAME=my_keras_model \\\n",
        "#    tensorflow/serving\n",
        "```\n",
        "\n",
        "### 3. Meng-query TF Serving Melalui REST API\n",
        "Setelah server berjalan, Anda dapat mengirim permintaan dari mana saja, termasuk dari notebook Colab.\n",
        "\n",
        "```python\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# Siapkan data input dummy\n",
        "X_new = np.random.rand(3, 8).astype(np.float32)\n",
        "\n",
        "# URL server (jika berjalan di mesin lokal)\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_keras_model:predict'\n",
        "\n",
        "# Membuat permintaan POST\n",
        "# Kita perlu mengubah data NumPy menjadi list Python agar bisa di-encode ke JSON\n",
        "input_data_json = json.dumps({\n",
        "    \"instances\": X_new.tolist()\n",
        "})\n",
        "\n",
        "try:\n",
        "    response = requests.post(SERVER_URL, data=input_data_json)\n",
        "    response.raise_for_status() # Lemparkan error jika status bukan 200\n",
        "    response_data = response.json()\n",
        "    y_pred = np.array(response_data[\"predictions\"])\n",
        "    print(\"Prediksi dari REST API:\\n\", y_pred)\n",
        "except requests.exceptions.ConnectionError as e:\n",
        "    print(\"Koneksi ke TF Serving gagal. Pastikan server Docker berjalan.\")\n",
        "    print(e)\n",
        "```\n",
        "\n",
        "### 4. Melatih di Beberapa GPU dengan `MirroredStrategy`\n",
        "Jika notebook Colab Anda berjalan pada *runtime* GPU, Anda bisa mensimulasikan penggunaan beberapa GPU atau menjalankannya jika tersedia lebih dari satu.\n",
        "\n",
        "```python\n",
        "# Membuat strategi distribusi\n",
        "# TensorFlow akan secara otomatis mendeteksi semua GPU yang tersedia\n",
        "try:\n",
        "    distribution = tf.distribute.MirroredStrategy()\n",
        "    print(\"Jumlah perangkat:\", distribution.num_replicas_in_sync)\n",
        "except RuntimeError as e:\n",
        "    print(\"MirroredStrategy hanya dapat digunakan dengan GPU atau TPU.\")\n",
        "    print(e)\n",
        "    distribution = None\n",
        "\n",
        "if distribution:\n",
        "    # Semua pembuatan model dan kompilasi harus berada di dalam scope strategi\n",
        "    with distribution.scope():\n",
        "        mirrored_model = keras.models.Sequential([\n",
        "            keras.layers.Dense(10, activation=\"relu\", input_shape=[8]),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "        mirrored_model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
        "\n",
        "    # Data dummy untuk training\n",
        "    X_train_dist = np.random.rand(100, 8)\n",
        "    y_train_dist = np.random.rand(100, 1)\n",
        "\n",
        "    # Training akan didistribusikan secara otomatis\n",
        "    # batch_size harus dapat dibagi rata dengan jumlah replika/GPU\n",
        "    # history = mirrored_model.fit(X_train_dist, y_train_dist, epochs=10,\n",
        "    #                                batch_size=32 * distribution.num_replicas_in_sync)\n",
        "    print(\"\\nModel dengan MirroredStrategy telah dibuat dan dikompilasi.\")\n",
        "    print(\"Proses training akan didistribusikan secara otomatis saat model.fit() dipanggil.\")\n",
        "```\n",
        "`MirroredStrategy` membuat proses training terdistribusi menjadi sangat sederhana. Model dan semua variabelnya direplikasi di setiap GPU, dan gradien disinkronkan di setiap langkah training.\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
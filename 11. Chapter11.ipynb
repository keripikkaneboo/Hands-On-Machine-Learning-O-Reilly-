{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd/IS4OwTgVaegnQPVqOAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/11.%20Chapter11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 11: Training Deep Neural Networks\n",
        "\n",
        "Melatih jaringan saraf yang dalam (DNN) bisa menjadi sulit. Bab ini membahas masalah-masalah utama yang sering muncul dan menyajikan berbagai teknik untuk mengatasinya.\n",
        "\n",
        "* **Masalah Gradien yang Lenyap/Meledak (*Vanishing/Exploding Gradients*)**: Selama *backpropagation*, gradien seringkali menjadi semakin kecil (lenyap) atau semakin besar (meledak) saat mengalir ke lapisan-lapisan bawah. Hal ini membuat lapisan-lapisan awal sangat sulit untuk dilatih. Beberapa solusi untuk masalah ini adalah:\n",
        "    * **Inisialisasi Bobot yang Lebih Baik**: Menggunakan metode inisialisasi seperti **Glorot (Xavier)** atau **He initialization** dapat membantu menjaga sinyal (dan gradien) mengalir dengan baik di seluruh jaringan.\n",
        "    * **Fungsi Aktivasi Non-Saturasi**: Menggunakan fungsi aktivasi seperti **ReLU** dan variannya (**Leaky ReLU, ELU, SELU**) membantu mengurangi masalah gradien yang lenyap karena mereka tidak jenuh pada nilai positif.\n",
        "    * **Batch Normalization (BN)**: Menambahkan lapisan BN di antara lapisan-lapisan lain. BN menormalisasi input di setiap lapisan, yang secara signifikan menstabilkan dan mempercepat training.\n",
        "    * **Gradient Clipping**: Membatasi nilai gradien agar tidak melebihi *threshold* tertentu, mencegah gradien meledak.\n",
        "\n",
        "* **Menggunakan Kembali Lapisan Pretrained (*Transfer Learning*)**: Daripada melatih DNN dari awal, seringkali lebih baik menggunakan kembali lapisan-lapisan bawah dari jaringan yang sudah dilatih pada dataset besar yang serupa. Ini mempercepat training secara dramatis dan membutuhkan lebih sedikit data.\n",
        "    * **Strategi**: \"Bekukan\" (*freeze*) lapisan-lapisan yang digunakan kembali pada awalnya, latih lapisan-lapisan atas yang baru, lalu \"cairkan\" (*unfreeze*) beberapa lapisan dan lanjutkan training dengan *learning rate* yang lebih rendah.\n",
        "\n",
        "* **Optimizer yang Lebih Cepat**: Menggunakan optimizer yang lebih baik daripada SGD standar dapat memberikan peningkatan kecepatan yang signifikan. Optimizer yang populer meliputi:\n",
        "    * **Momentum** dan **Nesterov Accelerated Gradient (NAG)**.\n",
        "    * **Optimizer dengan *Adaptive Learning Rate*** seperti **AdaGrad**, **RMSProp**, dan yang paling populer, **Adam** dan **Nadam**.\n",
        "\n",
        "* **Penjadwalan Learning Rate (*Learning Rate Scheduling*)**: Mengubah *learning rate* selama training dapat membantu konvergensi lebih cepat dan lebih baik daripada menggunakan *learning rate* konstan. Beberapa jadwal populer adalah *power scheduling*, *exponential scheduling*, dan **1cycle scheduling**.\n",
        "\n",
        "* **Teknik Regularisasi untuk Menghindari Overfitting**:\n",
        "    * **Regularisasi ℓ₁ dan ℓ₂**: Menambahkan penalti pada bobot model.\n",
        "    * **Dropout**: Teknik yang sangat populer di mana neuron secara acak \"dijatuhkan\" (diabaikan) selama setiap langkah training. Ini memaksa jaringan menjadi lebih kuat dan tidak terlalu bergantung pada neuron tertentu.\n",
        "    * **Max-Norm Regularization**: Membatasi norma ℓ₂ dari vektor bobot yang masuk ke setiap neuron.\n",
        "\n",
        "### 1. Inisialisasi He, Aktivasi ELU, dan Batch Normalization\n",
        "Berikut adalah contoh model yang menerapkan beberapa praktik terbaik untuk mengatasi gradien yang tidak stabil.\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "\n",
        "# Contoh model dengan inisialisasi He, aktivasi ELU, dan Batch Normalization\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(), # Menambahkan BN sebelum lapisan Dense\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "**Catatan**: Urutan antara lapisan aktivasi dan Batch Normalization masih menjadi perdebatan. Anda bisa menempatkan BN sebelum atau sesudah fungsi aktivasi.\n",
        "\n",
        "### 2. Gradient Clipping\n",
        "Untuk menerapkan *gradient clipping*, Anda cukup mengaturnya pada optimizer saat membuat instance.\n",
        "\n",
        "```python\n",
        "# Membuat optimizer dengan gradient clipping\n",
        "# clipvalue memotong setiap komponen gradien secara individual\n",
        "# clipnorm memotong seluruh norma gradien jika melebihi threshold\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "\n",
        "# Kemudian kompilasi model dengan optimizer ini\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)\n",
        "```\n",
        "\n",
        "### 3. Transfer Learning Menggunakan Keras\n",
        "Contoh ini menunjukkan cara menggunakan model Xception yang sudah dilatih pada ImageNet untuk tugas baru.\n",
        "\n",
        "```python\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Memuat model Xception pretrained tanpa lapisan atasnya\n",
        "# weights=\"imagenet\" akan mengunduh bobot yang telah dilatih\n",
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "\n",
        "# Menambahkan lapisan atas yang baru untuk tugas kita\n",
        "# Misalkan kita punya 5 kelas\n",
        "n_classes = 5\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model = keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# 1. Bekukan lapisan-lapisan dasar agar tidak ikut terlatih pada awalnya\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 2. Kompilasi dan latih hanya lapisan atas yang baru\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "# history = model.fit(train_set, epochs=5, validation_data=valid_set)\n",
        "\n",
        "# 3. Setelah beberapa epoch, cairkan lapisan dasar dan lanjutkan training\n",
        "#    dengan learning rate yang sangat kecil untuk fine-tuning\n",
        "print(\"\\nMelakukan fine-tuning...\")\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "# history = model.fit(train_set, epochs=5, validation_data=valid_set)\n",
        "```\n",
        "**Catatan**: Kode di atas hanya kerangka. Anda memerlukan `train_set` dan `valid_set` yang sesuai untuk menjalankannya.\n",
        "\n",
        "### 4. Optimizer Lanjutan dan Penjadwalan Learning Rate\n",
        "Menggunakan optimizer Adam dan jadwal *learning rate* eksponensial.\n",
        "\n",
        "```python\n",
        "# Menggunakan optimizer Adam (seringkali merupakan pilihan default yang baik)\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# Membuat fungsi jadwal learning rate\n",
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)\n",
        "\n",
        "# Membuat callback untuk learning rate scheduler\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "\n",
        "# Callback ini akan dipanggil saat melatih model\n",
        "# history = model.fit(X_train, y_train, ..., callbacks=[lr_scheduler])\n",
        "```\n",
        "\n",
        "### 5. Regularisasi dengan Dropout\n",
        "Menambahkan lapisan `Dropout` adalah cara yang efektif untuk mencegah *overfitting*.\n",
        "\n",
        "```python\n",
        "# Menambahkan lapisan Dropout di antara lapisan Dense\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2), # Menjatuhkan 20% neuron secara acak\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "# history = model.fit(...)\n",
        "```\n",
        "Dropout hanya aktif selama training dan dinonaktifkan selama inferensi (prediksi).\n",
        "\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6wpRSiSh1Rjo+I6B2/vQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/15.%20Chapter15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 15: Processing Sequences Using RNN and CNN\n",
        "\n",
        "Bab ini memperkenalkan **Recurrent Neural Networks (RNNs)**, sebuah kelas jaringan saraf yang dirancang khusus untuk menangani data sekuensial atau urutan, seperti data deret waktu (*time series*), teks, atau audio. Berbeda dengan jaringan saraf sebelumnya, RNN memiliki koneksi yang mengarah ke belakang, memberinya semacam \"memori\".\n",
        "\n",
        "* **Recurrent Neurons dan Layers**:\n",
        "    * Neuron rekuren menerima input dari langkah waktu saat ini dan outputnya sendiri dari langkah waktu sebelumnya. Ini memungkinkan jaringan untuk mengingat informasi dari masa lalu.\n",
        "    * Ketika RNN di-\"buka\" sepanjang waktu (*unrolled through time*), ia terlihat seperti jaringan saraf dalam biasa, di mana bobotnya digunakan bersama di setiap langkah waktu.\n",
        "    * **Memory Cell**: Bagian dari jaringan yang menyimpan *state* antar langkah waktu. Sel RNN sederhana hanya mampu mengingat pola jangka pendek.\n",
        "\n",
        "* **Arsitektur Input dan Output**: RNN sangat fleksibel:\n",
        "    * **Sequence-to-Sequence**: Menerima urutan dan menghasilkan urutan (misalnya, peramalan cuaca).\n",
        "    * **Sequence-to-Vector**: Menerima urutan dan menghasilkan satu vektor (misalnya, analisis sentimen).\n",
        "    * **Vector-to-Sequence**: Menerima satu vektor dan menghasilkan urutan (misalnya, membuat *caption* untuk gambar).\n",
        "    * **Encoder-Decoder**: Gabungan dari *sequence-to-vector* dan *vector-to-sequence* (misalnya, penerjemahan mesin).\n",
        "\n",
        "* **Tantangan dalam Melatih RNN**:\n",
        "    * **Gradien yang Tidak Stabil**: Sama seperti DNN biasa, RNN yang dalam (panjang) dapat menderita masalah *vanishing* atau *exploding gradient*.\n",
        "    * **Memori Jangka Pendek yang Terbatas**: Sel RNN sederhana cenderung \"melupakan\" input yang sudah sangat lama.\n",
        "\n",
        "* **Mengatasi Masalah Memori Jangka Pendek**:\n",
        "    * **LSTM (Long Short-Term Memory) Cells**: Sel yang lebih kompleks dengan *state* jangka panjang dan jangka pendek. Ia memiliki tiga \"gerbang\" (*gates*): *forget gate*, *input gate*, dan *output gate*, yang secara cerdas mengatur informasi apa yang harus disimpan, dibuang, dan dibaca.\n",
        "    * **GRU (Gated Recurrent Unit) Cells**: Varian yang lebih sederhana dari LSTM yang seringkali memberikan performa yang sama baiknya. Ia menggabungkan *state* jangka panjang dan pendek menjadi satu dan hanya menggunakan dua gerbang.\n",
        "\n",
        "* **Menggunakan CNN untuk Urutan**:\n",
        "    * **1D Convolutional Layers**: Lapisan konvolusional 1D dapat diterapkan pada urutan. Mereka sangat efisien dan dapat belajar mendeteksi pola lokal jangka pendek. Mereka bisa digunakan sebagai lapisan *preprocessing* untuk RNN (misalnya, untuk *downsampling*) atau digunakan sendiri.\n",
        "    * **WaveNet**: Arsitektur yang hanya menggunakan lapisan konvolusional 1D yang ditumpuk, dengan *dilation rate* yang meningkat secara eksponensial. Ini memungkinkan jaringan untuk memiliki *receptive field* yang sangat besar dan belajar pola jangka panjang dengan sangat efisien.\n",
        "\n",
        "### 1. Meramalkan Deret Waktu dengan RNN Sederhana\n",
        "Kita akan membuat data deret waktu sintetis dan mencoba meramalkan nilai berikutnya.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fungsi untuk membuat dataset deret waktu\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "# Membuat data training, validasi, dan testing\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "# Membangun model RNN sederhana\n",
        "# Keras secara default hanya mengembalikan output dari langkah waktu terakhir\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "# Menggunakan Dense layer sebagai output lebih fleksibel\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20), # Hanya mengembalikan output terakhir\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "# history = model.fit(X_train, y_train, epochs=20,\n",
        "#                     validation_data=(X_valid, y_valid))\n",
        "print(\"Model RNN untuk peramalan satu langkah telah dibuat.\")\n",
        "```\n",
        "\n",
        "### 2. Meramalkan Beberapa Langkah ke Depan (Sequence-to-Sequence)\n",
        "Kita bisa melatih RNN untuk memprediksi 10 nilai berikutnya di setiap langkah waktu. Ini menstabilkan dan mempercepat training.\n",
        "\n",
        "```python\n",
        "# Mempersiapkan target: setiap target adalah urutan 10D\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, n_steps:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, n_steps:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, n_steps:, 0]\n",
        "\n",
        "# Membangun model Seq-to-Seq\n",
        "# return_sequences=True di semua lapisan RNN\n",
        "# TimeDistributed menerapkan Dense layer ke setiap langkah waktu\n",
        "model_seq = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# Fungsi loss kustom untuk hanya mengevaluasi langkah waktu terakhir\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
        "\n",
        "model_seq.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "                  metrics=[last_time_step_mse])\n",
        "\n",
        "# history = model_seq.fit(X_train, Y_train, epochs=20,\n",
        "#                         validation_data=(X_valid, Y_valid))\n",
        "print(\"\\nModel Seq-to-Seq telah dibuat.\")\n",
        "```\n",
        "\n",
        "### 3. LSTM dan GRU\n",
        "Untuk menangani ketergantungan jangka panjang, kita bisa mengganti `SimpleRNN` dengan `LSTM` atau `GRU`. Penggunaannya di Keras sangat mudah.\n",
        "\n",
        "```python\n",
        "# Model dengan GRU (penggunaannya sama dengan LSTM)\n",
        "model_gru = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# Kompilasi dan training sama seperti sebelumnya\n",
        "# model_gru.compile(...)\n",
        "# model_gru.fit(...)\n",
        "print(\"\\nModel dengan lapisan GRU telah dibuat.\")\n",
        "```\n",
        "\n",
        "### 4. WaveNet Menggunakan Lapisan Konvolusional 1D\n",
        "Contoh arsitektur yang terinspirasi dari WaveNet menggunakan tumpukan lapisan `Conv1D` dengan `dilation_rate` yang meningkat.\n",
        "\n",
        "```python\n",
        "# Mempersiapkan data Y yang sama panjangnya dengan X\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train, Y_valid = Y[:7000], Y[7000:9000]\n",
        "\n",
        "# Model WaveNet\n",
        "model_wavenet = keras.models.Sequential()\n",
        "model_wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2: # Tumpukan blok dilatasi\n",
        "    model_wavenet.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                 activation=\"relu\", dilation_rate=rate))\n",
        "model_wavenet.add(keras.layers.Conv1D(filters=10, kernel_size=1)) # Lapisan output\n",
        "\n",
        "model_wavenet.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "\n",
        "# history = model_wavenet.fit(X_train, Y_train, epochs=20,\n",
        "#                             validation_data=(X_valid, Y_valid))\n",
        "print(\"\\nModel WaveNet telah dibuat.\")\n",
        "```\n",
        "Lapisan konvolusional 1D dengan *causal padding* dan *dilation rate* yang meningkat memungkinkan model untuk melihat pola jangka panjang secara efisien tanpa memerlukan komputasi sekuensial dari RNN.\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
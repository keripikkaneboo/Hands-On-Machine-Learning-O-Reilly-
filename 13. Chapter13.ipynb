{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO5048ISG6KX7AGXfxZY9g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/13.%20Chapter13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "Saat bekerja dengan dataset yang sangat besar yang tidak muat dalam memori, kita memerlukan cara yang efisien untuk memuat dan memproses data. Bab ini memperkenalkan **Data API (`tf.data`)** dari TensorFlow, sebuah alat yang kuat untuk membangun pipeline input yang skalabel dan beperforma tinggi.\n",
        "\n",
        "* **Data API (`tf.data`)**:\n",
        "    * **Konsep Inti**: API ini berpusat pada objek `tf.data.Dataset`, yang merepresentasikan sebuah urutan item data.\n",
        "    * **Pipeline Efisien**: Anda dapat membuat pipeline dengan merangkai (*chaining*) beberapa metode transformasi pada dataset, seperti:\n",
        "        * `map()`: Menerapkan fungsi preprocessing pada setiap item.\n",
        "        * `shuffle()`: Mengacak item data.\n",
        "        * `batch()`: Mengelompokkan item ke dalam *batch*.\n",
        "        * `prefetch()`: Memungkinkan data disiapkan di latar belakang saat model sedang training, mencegah GPU menunggu data dan meningkatkan utilisasi.\n",
        "        * `cache()`: Menyimpan dataset dalam memori setelah pemrosesan pertama, mempercepat epoch-epoch berikutnya (berguna untuk dataset yang tidak terlalu besar).\n",
        "    * **Membaca dari Berbagai Sumber**: Data API dapat membaca data dari file teks (CSV), file biner, dan format `TFRecord`, serta mendukung pembacaan dari banyak file secara paralel.\n",
        "\n",
        "* **Format TFRecord**:\n",
        "    * Format biner pilihan TensorFlow untuk menyimpan data dalam jumlah besar dan membacanya secara efisien.\n",
        "    * TFRecord pada dasarnya adalah daftar rekaman biner, di mana setiap rekaman biasanya berisi **protocol buffer** yang diserialisasi.\n",
        "    * **Protocol Buffer `Example`**: Protobuf standar yang digunakan di TensorFlow. Ia bertindak sebagai kamus (`map`) yang memetakan nama fitur ke nilainya (`BytesList`, `FloatList`, atau `Int64List`).\n",
        "\n",
        "* **Lapisan Preprocessing Keras**:\n",
        "    * Cara modern dan direkomendasikan untuk melakukan preprocessing adalah dengan memasukkannya sebagai lapisan di awal model Anda. Ini memiliki beberapa keuntungan:\n",
        "        1.  Kode preprocessing hanya perlu ditulis sekali.\n",
        "        2.  Model yang disimpan sudah mencakup preprocessing, sehingga mudah untuk dideploy dan mengurangi risiko *training/serving skew* (perbedaan antara preprocessing saat training dan saat inferensi).\n",
        "    * **Lapisan yang Tersedia**:\n",
        "        * **Encoding Fitur Kategorikal**:\n",
        "            * `TextVectorization`: Mengubah teks menjadi urutan integer (indeks kata).\n",
        "            * `Embedding`: Mengubah indeks integer menjadi vektor padat (*dense vector*) yang dapat dilatih. Ini adalah cara yang efisien untuk merepresentasikan kategori, terutama untuk kosa kata yang besar.\n",
        "        * **Normalisasi Fitur**:\n",
        "            * `Normalization`: Menstandardisasi fitur numerik (mengurangi rata-rata dan membagi dengan standar deviasi).\n",
        "    * **Metode `adapt()`**: Lapisan preprocessing ini memiliki metode `adapt()` yang memungkinkan mereka mempelajari statistik yang diperlukan dari sampel data (misalnya, mempelajari kosa kata dari teks atau rata-rata dan standar deviasi dari fitur numerik).\n",
        "\n",
        "* **TensorFlow Datasets (TFDS)**:\n",
        "    * Sebuah library yang menyediakan akses mudah ke ratusan dataset umum.\n",
        "    * Fungsi `tfds.load()` mengunduh data dan mengembalikannya sebagai `tf.data.Dataset`, siap untuk diproses lebih lanjut.\n",
        "\n",
        "### 1. Membangun Pipeline Input dengan Data API\n",
        "Contoh dasar membuat pipeline `tf.data`.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Membuat dataset dari tensor di memori\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "print(\"Dataset awal:\", list(dataset.as_numpy_iterator()))\n",
        "\n",
        "# Merangkai transformasi\n",
        "dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
        "print(\"\\nDataset setelah repeat(3) dan batch(7):\")\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "\n",
        "# Transformasi lebih lanjut dengan map, shuffle, dan prefetch\n",
        "dataset = dataset.map(lambda x: x * 2) # Mengalikan setiap elemen dengan 2\n",
        "dataset = dataset.unbatch() # Mengembalikan ke elemen individual\n",
        "dataset = dataset.shuffle(buffer_size=10).batch(5).prefetch(1)\n",
        "\n",
        "print(\"\\nPipeline akhir:\")\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "```\n",
        "\n",
        "### 2. Menulis dan Membaca File TFRecord\n",
        "Contoh ini menunjukkan cara membuat file `.tfrecord` dari data, lalu membacanya kembali.\n",
        "\n",
        "```python\n",
        "# Menulis ke file TFRecord\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"Ini adalah rekaman pertama.\")\n",
        "    f.write(b\"Dan ini yang kedua.\")\n",
        "\n",
        "# Membaca dari file TFRecord\n",
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "\n",
        "# --- Contoh dengan `Example` Protobuf ---\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "# Membuat `Example` protobuf\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
        "        }))\n",
        "\n",
        "# Menulis Example yang diserialisasi ke file TFRecord\n",
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString())\n",
        "\n",
        "# Membaca dan mem-parsing Example\n",
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"])\n",
        "for serialized_example in dataset:\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
        "                                                 feature_description)\n",
        "    print(\"\\nParsed Example:\", parsed_example)\n",
        "```\n",
        "\n",
        "### 3. Menggunakan Lapisan Preprocessing Keras\n",
        "Contoh bagaimana lapisan `TextVectorization` dan `Embedding` digunakan dalam sebuah model.\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Contoh data teks\n",
        "some_texts = np.array([\"I love deep learning\", \"deep learning is awesome\"])\n",
        "\n",
        "# 1. Buat dan adaptasi TextVectorization layer\n",
        "# max_tokens: ukuran kosa kata, output_sequence_length: panjang urutan output\n",
        "text_vec_layer = keras.layers.TextVectorization(max_tokens=100, output_sequence_length=5)\n",
        "text_vec_layer.adapt(some_texts)\n",
        "\n",
        "print(\"\\nKosa kata:\", text_vec_layer.get_vocabulary())\n",
        "print(\"Hasil vectorization:\", text_vec_layer(some_texts))\n",
        "\n",
        "\n",
        "# 2. Menggunakan Embedding dalam sebuah model\n",
        "embedding_dim = 2\n",
        "vocab_size = text_vec_layer.vocabulary_size()\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    text_vec_layer, # Lapisan preprocessing\n",
        "    keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "])\n",
        "\n",
        "# Menjalankan model untuk melihat hasil embedding\n",
        "embedding_output = model.predict(some_texts)\n",
        "print(\"\\nOutput embedding:\\n\", embedding_output)\n",
        "```\n",
        "\n",
        "### 4. Menggunakan TensorFlow Datasets (TFDS)\n",
        "Cara termudah untuk mendapatkan akses ke ratusan dataset standar.\n",
        "\n",
        "```python\n",
        "# Diperlukan untuk Google Colab\n",
        "!pip install -q -U tensorflow_datasets\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Memuat dataset imdb_reviews, sudah dibagi dan dibatch\n",
        "# as_supervised=True akan mengembalikan tuple (input, label)\n",
        "dataset, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "train_set = dataset[\"train\"].batch(32).prefetch(1)\n",
        "\n",
        "print(\"\\nInfo dataset IMDb:\", info.splits)\n",
        "\n",
        "# Dataset ini sekarang siap untuk dimasukkan ke model Keras\n",
        "# for X_batch, y_batch in train_set.take(1):\n",
        "#     print(X_batch.shape, y_batch.shape)\n",
        "```\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNThhv13F6PIiK3MY8TRpI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/10.%20Chapter10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 10: Introduction to Artificial Neural Networks with Keras\n",
        "\n",
        "Bab ini memperkenalkan konsep dasar di balik **Artificial Neural Networks (ANN)**, mulai dari inspirasi biologisnya hingga implementasi praktis menggunakan API tingkat tinggi, **Keras**.\n",
        "\n",
        "* **Dari Neuron Biologis ke Neuron Buatan**: Bab ini dimulai dengan sejarah singkat ANN, terinspirasi dari cara kerja otak. Diperkenalkan arsitektur paling awal seperti **Perceptron**, yang merupakan sebuah unit komputasi sederhana (*Threshold Logic Unit* atau TLU) yang dapat melakukan klasifikasi biner linier. Namun, Perceptron memiliki keterbatasan signifikan (misalnya, tidak bisa menyelesaikan masalah XOR), yang menyebabkan periode stagnasi dalam riset ANN (\"AI winter\").\n",
        "\n",
        "* **Multilayer Perceptron (MLP) dan Backpropagation**:\n",
        "    * **MLP** adalah solusi untuk keterbatasan Perceptron, yang terdiri dari satu *input layer*, satu atau lebih *hidden layers*, dan satu *output layer*. Dengan lapisan yang cukup, MLP dapat memecahkan masalah yang sangat kompleks.\n",
        "    * **Backpropagation** adalah algoritma training fundamental untuk MLP. Ia bekerja dengan cara:\n",
        "        1.  Melakukan *forward pass* untuk menghitung output dan *loss*.\n",
        "        2.  Melakukan *reverse pass* untuk mengukur kontribusi error dari setiap koneksi.\n",
        "        3.  Menggunakan gradien yang dihitung untuk memperbarui bobot model melalui *Gradient Descent*.\n",
        "    * **Fungsi Aktivasi**: Diperkenalkan fungsi aktivasi non-linier seperti **ReLU**, **tanh**, dan **sigmoid**, yang sangat penting karena tanpa non-linearitas, menumpuk beberapa lapisan sama saja dengan satu lapisan linier.\n",
        "\n",
        "* **Implementasi MLP dengan Keras**:\n",
        "    * **Keras** adalah API tingkat tinggi yang intuitif untuk membangun, melatih, dan mengevaluasi jaringan saraf. `tf.keras` adalah implementasi Keras yang terintegrasi penuh dengan TensorFlow.\n",
        "    * **Sequential API**: Cara paling sederhana untuk membangun model, yaitu dengan membuat tumpukan lapisan (*stack of layers*) secara berurutan.\n",
        "    * **Functional API**: Memungkinkan pembangunan arsitektur yang lebih kompleks, seperti jaringan dengan beberapa input atau output (contohnya, arsitektur *Wide & Deep*).\n",
        "    * **Subclassing API**: Memberikan fleksibilitas maksimum dengan memungkinkan kita membuat model sebagai *class* Python, berguna untuk model yang sangat dinamis.\n",
        "\n",
        "* **Praktik Terbaik**:\n",
        "    * **Menyimpan dan Memuat Model**: Keras memudahkan penyimpanan arsitektur model, bobot, dan status optimizer.\n",
        "    * **Callbacks**: Fungsi yang dapat dieksekusi pada berbagai titik selama training, seperti `ModelCheckpoint` untuk menyimpan model terbaik dan `EarlyStopping` untuk menghentikan training lebih awal.\n",
        "    * **TensorBoard**: Alat visualisasi untuk memantau metrik training, melihat arsitektur model, dan lainnya.\n",
        "    * ***Hyperparameter Tuning***: Teknik untuk menemukan kombinasi *hyperparameter* terbaik (misalnya, jumlah lapisan, jumlah neuron, *learning rate*) menggunakan *wrapper* Scikit-Learn dan alat seperti `RandomizedSearchCV`.\n",
        "\n",
        "### 1. Membangun Pengklasifikasi Gambar dengan Sequential API\n",
        "Kita akan menggunakan dataset Fashion MNIST, versi yang lebih menantang dari MNIST.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Memuat dataset Fashion MNIST\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Membagi data training menjadi training dan validation set\n",
        "# dan melakukan normalisasi piksel (scaling) ke rentang 0-1\n",
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "# Membangun model menggunakan Sequential API\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]), # Meratakan input 28x28 menjadi vektor 1D\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\") # Output layer dengan 10 neuron (1 per kelas)\n",
        "])\n",
        "\n",
        "# Menampilkan ringkasan model\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "### 2. Mengompilasi dan Melatih Model\n",
        "Setelah model dibuat, kita perlu mengompilasinya dengan menentukan *loss function*, *optimizer*, dan metrik, lalu melatihnya.\n",
        "\n",
        "```python\n",
        "# Mengompilasi model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Melatih model\n",
        "# history akan menyimpan metrik training dan validasi di setiap epoch\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Plot learning curves\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # Mengatur batas vertikal [0, 1]\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 3. Evaluasi dan Prediksi\n",
        "Setelah model dilatih, kita bisa mengevaluasinya pada *test set* dan menggunakannya untuk prediksi.\n",
        "\n",
        "```python\n",
        "# Evaluasi pada test set\n",
        "print(\"Hasil evaluasi pada test set:\")\n",
        "model.evaluate(X_test, y_test)\n",
        "\n",
        "# Membuat prediksi pada beberapa instance baru\n",
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_pred = np.argmax(y_proba, axis=-1)\n",
        "\n",
        "print(\"\\nPrediksi probabilitas:\\n\", y_proba.round(2))\n",
        "print(\"Prediksi kelas:\", np.array(class_names)[y_pred])\n",
        "```\n",
        "\n",
        "### 4. Membangun Model Regresi dengan Functional API\n",
        "Functional API lebih fleksibel dan berguna untuk arsitektur yang lebih kompleks, seperti model *Wide & Deep*.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Memuat dan mempersiapkan dataset California Housing\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Membangun model Wide & Deep dengan Functional API\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_, hidden2]) # Skip connection (jalur 'wide')\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "# Mengompilasi dan melatih model\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "print(\"\\nMSE pada test set:\", mse_test)\n",
        "```\n",
        "Contoh di atas menunjukkan cara membuat model dengan koneksi non-sekuensial (*skip connection*), yang merupakan keunggulan utama dari Functional API.\n",
        "\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjVBBfNSIvD0s3vmNSvO27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keripikkaneboo/Hands-On-Machine-Learning-O-Reilly-/blob/main/07.%20Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bab 7: Ensemble Learning dan Random Forests\n",
        "\n",
        "Bab ini membahas **Ensemble Learning**, sebuah teknik di mana beberapa model (*predictors*) digabungkan untuk menghasilkan prediksi yang lebih baik daripada model tunggal mana pun. Idenya mirip dengan \"kebijaksanaan orang banyak\" (*wisdom of the crowd*): jawaban kolektif seringkali lebih baik daripada jawaban seorang ahli.\n",
        "\n",
        "* **Voting Classifiers**: Cara paling sederhana untuk membuat *ensemble*. Beberapa model yang berbeda dilatih, dan prediksi akhir ditentukan oleh suara mayoritas.\n",
        "    * ***Hard Voting***: Prediksi akhir adalah kelas yang paling banyak dipilih oleh para pengklasifikasi.\n",
        "    * ***Soft Voting***: Menghitung rata-rata probabilitas kelas dari semua pengklasifikasi dan memilih kelas dengan probabilitas tertinggi. Biasanya memberikan performa lebih baik jika para pengklasifikasi dapat mengestimasi probabilitas.\n",
        "\n",
        "* **Bagging dan Pasting**: Teknik *ensemble* di mana algoritma yang sama dilatih pada *subset* acak yang berbeda dari data training.\n",
        "    * ***Bagging*** (*Bootstrap Aggregating*): Pengambilan sampel *subset* dilakukan **dengan** pengembalian (*with replacement*).\n",
        "    * ***Pasting***: Pengambilan sampel *subset* dilakukan **tanpa** pengembalian (*without replacement*).\n",
        "    * Teknik ini sangat baik untuk mengurangi varians model dan mudah diparalelkan.\n",
        "    * ***Out-of-Bag (oob) Evaluation***: Karena *bagging* mengambil sampel dengan pengembalian, beberapa instance mungkin tidak pernah terpilih untuk dilatih oleh sebuah *predictor*. Instance-instance ini (disebut *out-of-bag*) dapat digunakan sebagai *validation set* tanpa perlu memisahkan data secara manual.\n",
        "\n",
        "* **Random Forests**: Adalah sebuah *ensemble* dari Decision Trees, yang biasanya dilatih melalui metode *bagging*. Keunikan utamanya adalah selain mengambil sampel instance secara acak, ia juga mengambil sampel fitur secara acak di setiap *split node*. Hal ini menghasilkan pohon yang lebih beragam dan model yang lebih kuat.\n",
        "    * ***Feature Importance***: Random Forest juga menyediakan cara mudah untuk mengukur pentingnya relatif dari setiap fitur.\n",
        "\n",
        "* **Boosting**: Teknik *ensemble* di mana *predictor* dilatih secara sekuensial, dengan setiap *predictor* baru mencoba untuk memperbaiki kesalahan dari *predictor* sebelumnya.\n",
        "    * ***AdaBoost (Adaptive Boosting)***: Fokus pada instance yang salah diklasifikasikan oleh *predictor* sebelumnya dengan cara meningkatkan bobotnya.\n",
        "    * ***Gradient Boosting***: Melatih setiap *predictor* baru pada *residual error* (selisih antara target sebenarnya dan prediksi) dari *predictor* sebelumnya.\n",
        "\n",
        "* **Stacking** (*Stacked Generalization*): Alih-alih menggunakan voting, sebuah model akhir (disebut *blender* atau *meta-learner*) dilatih untuk melakukan agregasi prediksi dari semua *predictor* di dalam *ensemble*.\n",
        "\n",
        "### 1. Voting Classifier\n",
        "Menggabungkan beberapa model klasifikasi yang berbeda untuk mendapatkan prediksi yang lebih baik.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Membuat dataset dan membaginya\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Inisialisasi beberapa model\n",
        "log_clf = LogisticRegression(random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42) # probability=True untuk soft voting\n",
        "\n",
        "# Membuat Voting Classifier\n",
        "# voting='hard' untuk mayoritas suara\n",
        "# voting='soft' untuk rata-rata probabilitas\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Membandingkan akurasi setiap model\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
        "```\n",
        "Biasanya, akurasi `VotingClassifier` akan sedikit lebih tinggi daripada model individual terbaik.\n",
        "\n",
        "### 2. Bagging dan Out-of-Bag (oob) Evaluation\n",
        "Melatih banyak Decision Tree pada subset acak dari data training.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Membuat ensemble dengan 500 Decision Tree\n",
        "# max_samples=100: setiap tree dilatih pada 100 instance acak\n",
        "# bootstrap=True: sampling dengan pengembalian (bagging)\n",
        "# oob_score=True: meminta evaluasi oob otomatis setelah training\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, oob_score=True, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Skor oob adalah estimasi akurasi pada data yang tidak terlihat\n",
        "print(\"OOB score:\", bag_clf.oob_score_)\n",
        "\n",
        "# Membandingkan dengan akurasi pada test set\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "Skor OOB seringkali menjadi estimasi yang baik untuk performa model pada *test set*.\n",
        "\n",
        "### 3. Random Forest dan Feature Importance\n",
        "`RandomForestClassifier` adalah versi yang lebih mudah digunakan dan dioptimalkan dari `BaggingClassifier` dengan `DecisionTreeClassifier`.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Inisialisasi Random Forest\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "print(\"Random Forest accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "\n",
        "# Menghitung feature importance pada dataset Iris\n",
        "iris = load_iris()\n",
        "rnd_clf_iris = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "rnd_clf_iris.fit(iris[\"data\"], iris[\"target\"])\n",
        "\n",
        "print(\"\\nFeature importances on Iris dataset:\")\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf_iris.feature_importances_):\n",
        "    print(name, score)\n",
        "```\n",
        "*Feature importance* menunjukkan fitur mana yang paling berpengaruh dalam membuat keputusan, yang sangat berguna untuk analisis.\n",
        "\n",
        "### 4. Boosting\n",
        "Contoh implementasi **AdaBoost**.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Menggunakan Decision Tree dangkal (stump) sebagai base estimator\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ada = ada_clf.predict(X_test)\n",
        "print(\"\\nAdaBoost accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
        "```\n",
        "Boosting melatih model secara sekuensial, sehingga cenderung lebih lambat tetapi seringkali menghasilkan model yang sangat akurat.\n",
        "\n"
      ],
      "metadata": {
        "id": "-0aEJqXVt4eB"
      }
    }
  ]
}